{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Adam\n",
    "\n",
    "The purpose of this notebook will be to understand the Adam optimizer as it is implemented in PyTorch.\n",
    "The original paper describing this algorithm can be found at: https://arxiv.org/pdf/1412.6980.pdf\n",
    "\n",
    "First of all, here is the pseudocode for the algorithm, taken from the original paper. It is essentially just your vanilla backpropagation with some added weights and correction terms. We'll get into the intuition for this extra steps momentarily.\n",
    "![Adam Pseudocode](pics/Adam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the idea here? Well, $g_t$ is just your normal computation of gradients using chain rule. The term $m_t$ is the mean error over $t$ steps, biased towards the initial vector. Since we chose the initial vector to be 0, this means that the algorithm is biased towards a zero gradient. This will get corrected later.\n",
    "\n",
    "The term $v_t$ is similarly a zero-biased estimate of the uncentered variance in the network's error. This estimate uses the assumption that the expectation of the squared error is the same as the square of the expected error.\n",
    "\n",
    "The bias-corrected estimates for $m_t$ and $v_t$ are $\\hat{m}_t$ and $\\hat{v}_t$. The paper claims that the ratio $\\hat{m}_t/\\sqrt{\\hat{v}_t}$ can be thought of like a signal-to-noise ratio (SNR), so that the smaller the SNR the smaller the step size in parameter space will be. This acts somewhat like automatic annealing because the SNR decreases close to an optimal point in parameter space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in PyTorch\n",
    "\n",
    "The implementation of Adam in PyTorch makes use of two improvements proposed in https://arxiv.org/pdf/1711.05101.pdf\n",
    "\n",
    "The first improvement is to use $$g_t ðŸ ¤ \\nabla_\\theta f_t(\\theta_{t-1})+\\lambda\\theta_{t-1}$$ in the first update rule. This function of this term is to essentially apply $L_2$ regularization in concert with the Adam update rule.\n",
    "\n",
    "The second improvement is to use $$\\theta_t ðŸ ¤ \\theta_{t-1}-\\eta_t(\\alpha \\hat{m}_t/(\\sqrt{\\hat{v}_t}+\\epsilon)+\\lambda\\theta_{t-1})$$ with scheduled annealing parameter $\\eta_t$. This addition of $\\lambda\\theta_{t-1}$ provides weight decay separately from the update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the source code from PyTorch implementing this improved Adam algorithm called AdamW, the default implementation of Adam used when `Adam` is called from `Optimizer`.\n",
    "\n",
    "I have removed all the safety checks and extra options from the code to make it as easy to read as possible. Please don't use this code outside this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "def adam(params: List[Tensor],           # list of parameters that contribute gradients\n",
    "         grads: List[Tensor],            # g_t\n",
    "         exp_avgs: List[Tensor],         # m_t\n",
    "         exp_avg_sqs: List[Tensor],      # v_t\n",
    "         max_exp_avg_sqs: List[Tensor],  # max(v_t)\n",
    "         state_steps: List[int],\n",
    "         amsgrad: bool,                  # I have removed this option, please ignore this parameter\n",
    "         beta1: float,                   # from pseudocode\n",
    "         beta2: float,                   # also from pseudocode\n",
    "         lr: float,                      # alpha from pseudocode\n",
    "         weight_decay: float,            # lambda from pseudocode\n",
    "         eps: float):                    # from pseudocode\n",
    "    r\"\"\"Functional API that performs Adam algorithm computation.\n",
    "    See :class:`~torch.optim.Adam` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        step = state_steps[i]\n",
    "        if amsgrad:\n",
    "            max_exp_avg_sq = max_exp_avg_sqs[i]\n",
    "\n",
    "        bias_correction1 = 1 - beta1 ** step\n",
    "        bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            # This is the first of the improvements in AdamW algorithm, the L2 regularization\n",
    "            grad = grad.add(param, alpha=weight_decay)\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)                     # update rule for m_t\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)        # update rule for v_t\n",
    "        denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps) # all the weights and biases go here\n",
    "\n",
    "        step_size = lr / bias_correction1                                   # add the learning rate alpha\n",
    "\n",
    "        param.addcdiv_(exp_avg, denom, value=-step_size)  # update params according to p -= step_size*exp_avg/denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the algorithm implemented, we just need to use the class `Adam` which has all the methods PyTorch expects from an optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)  # initialize using the inherited Optimizer class\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    @torch.no_grad()  # Makes every computation have requires_grad=False, since we already have all the grads we need\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []  # list of parameters that contribute gradients\n",
    "            grads = []             # g_t\n",
    "            exp_avgs = []          # m_t\n",
    "            exp_avg_sqs = []       # v_t\n",
    "            state_sums = []        #\n",
    "            max_exp_avg_sqs = []   #\n",
    "            state_steps = []       #\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    if p.grad.is_sparse:\n",
    "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                    grads.append(p.grad)  # initialize to zero\n",
    "\n",
    "                    state = self.state[p] # parameters in PyTorch have an attribute `state` which is a dict\n",
    "                    \n",
    "                    # Lazy state initialization, used in first round of updates. Can ignore this if reading code to learn.\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = 0\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if group['amsgrad']:\n",
    "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "                    if group['amsgrad']:\n",
    "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                    # update the steps for each param group update\n",
    "                    state['step'] += 1\n",
    "                    # record the step after step update\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            beta1, beta2 = group['betas'] # get the beta values for the algorithm from the class attribute 'betas'\n",
    "            \n",
    "            # call the adam update algorithm\n",
    "            adam(params_with_grad,\n",
    "                   grads,\n",
    "                   exp_avgs,\n",
    "                   exp_avg_sqs,\n",
    "                   max_exp_avg_sqs,\n",
    "                   state_steps,\n",
    "                   group['amsgrad'],\n",
    "                   beta1,\n",
    "                   beta2,\n",
    "                   group['lr'],\n",
    "                   group['weight_decay'],\n",
    "                   group['eps']\n",
    "                   )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick example\n",
    "\n",
    "Let's quickly check that this implementation is working. The following code should run without errors if our Adam code is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 63.04720687866211\n",
      "199 1.6846511363983154\n",
      "299 0.009795689955353737\n",
      "399 0.00013946012768428773\n",
      "499 1.3764803952653892e-05\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use our implementation of Adam\n",
    "learning_rate = 1e-4\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
