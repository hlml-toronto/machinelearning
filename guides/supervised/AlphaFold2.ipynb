{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaFold 2\n",
    "\n",
    "This notebook contains notes on the AlphaFold 2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "First, let's look at a high level at the AlphFold2 model.\n",
    "\n",
    "The required input for AlphaFold2 is a query amino acid sequence representing the protein of interest.\n",
    "The target output from evaluating AlphaFold2 with this query is a set of atom coordinates $\\{\\vec{x}_i^a\\}$ and the per-residue confidence $\\{p_i^{pLDDT}\\}$.\n",
    "\n",
    "\n",
    "The neural network part of AlphaFold2 doesn't take just the query amino acid sequence as input, but rather makes use of the wealth of knowledge that we already have about protein structure. This is done by incorporating evolutionarily related sequences from other species in a Multiple Sequence Alignment (MSA). Features are computed for the query sequence on the basis of this MSA, and these features are used to represent the query sequence.\n",
    "\n",
    "We'll start by inspecting the data pre-processing steps that happen immediately following the specification of a query sequence. \n",
    "![Architecture1](pics/AlphaFold_1.png)\n",
    "The query pre-processing steps generate two main inputs to the neural network. First, the MSA itself is embedded in a vector space and used as input. Second, the query sequence is represented as a residue pair matrix using features computed in the context of the MSA and structural templates selected from the PDB.\n",
    "\n",
    "These two inputs (the MSA embedding and the pair representation) are then used as input to the first neural network, the Evoformer. This network performs an additional embedding and outputs a new MSA embedding and new pair representation. These are used as input to the second neural network, the Structure Module. This module outputs the predicted atomic coordinates and per-residue confidence.\n",
    "\n",
    "The network block is evaluated iteratively, including current predictions as input for the next iteration. The final structure prediction is only achieved after many iterations.\n",
    "\n",
    "![Architecture2](pics/AlphaFold_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a closer look at the data pre-processing steps.\n",
    "\n",
    "## 1.1 Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nres = 256  # residues in the human primary sequence\n",
    "Ntempl = 4  # number of templates\n",
    "Nall_seq = 1152  # number of MSA sequences\n",
    "Nclust = 124  # number of MSA clusters\n",
    "Nseq = 128  # Nclust + Ntempl\n",
    "Nextra_seq = 1024  # unclustered MSA sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Capitalized function names contain trainable parameters.\n",
    "- Lower case function names have no parameters, but simply perform a transformation of the input.\n",
    "- Indices (i, j, k) are reserved for residue dimensions.\n",
    "- Indices (s, t) are reserved for sequence dimensions.\n",
    "- Index (h) is reserved for indexing the attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data pipeline\n",
    "\n",
    "The data pipeline is the first step when running AlphaFold. It takes an input FASTA file and produces the input features for the ML model.\n",
    "\n",
    "Several genetic searches are performed (using JackHMMER and HHBlits on different databases) and the output MSAs are stacked. \n",
    "\n",
    "Structural templates (i.e., 3D atomic coordinates to use as priors) are retrieved from the PDB using the MSA sequences as queries. The top 4 templates are input the model. These top templates are selected by the expected number of correctly aligned residues in the MSA. \n",
    "\n",
    "### Training filters\n",
    "Several filters are applied to the training data to help generate diversity and avoid particular edge cases.\n",
    "\n",
    "- Some QA is applied to remove particularly uncommon sequences. For example, sequences are rejected if any single a.a. accounts for more than 80\\% of the sequence.\n",
    "- Probabilistic rejection of training sequences based on length helps to rebalance the length distribution in training data\n",
    "- MSA block deletion: contiguous blocks of sequences in the MSA are randomly deleted, which tends to remove phylogenetic branches since MSA ordering typically places evolutionarily similar sequences contiguously\n",
    "- During training, fewer than 4 structural templates (even 0 templates) can be randomly selected, to try and avoid the ML model simply copying the template structure. \n",
    "\n",
    "### MSA clustering\n",
    "The peak computational complexity of the ML model scales as $\\mathcal{O}(N_{seq}^2\\times N_{res})$ so it is valuable to reduce the number of sequences in the MSA using unsupervised reduction. The procedure of reducing the MSA size is as follows:\n",
    "\n",
    "1. $N_{clust}$ sequences are selected with random uniform probability to be cluster centers, although the query sequence (e.g. human) is always one of the cluster centers.\n",
    "2. A mask is applied along the residue dimension of the MSA cluster center sequences, which is used later in the model, but involves random perturbations to the sequence.\n",
    "3. The remaining sequences in the MSA (i.e., not cluster centers) are assigned to the closest cluster center as measured by Hamming distance on un-masked residues. \n",
    "4. For each cluster, statistical features such as the per-residue a.a. distribution are computed and these features are also input to the ML model.\n",
    "5. An additional $N_{extra\\_seq}$ are selected from the MSA in addition to the cluster centers, and these sequences are input to the model. The remaining MSA sequences only enter the model through their contribution to the per-residue statistical summary features.\n",
    "\n",
    "During training, the number of residues is cropped to further reduce the computational complexity per training sample. This is not done during evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aargh', 'abaca', 'abaci']\n"
     ]
    }
   ],
   "source": [
    "with open(\"five_letter_words.txt\", 'r') as f:\n",
    "    five_letter_words = f.readlines()\n",
    "five_letter_words = sorted(five_letter_words)\n",
    "five_letter_words = list(map(str.strip, five_letter_words))\n",
    "print(five_letter_words[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word:  sudsy\n",
      "Query word's cluster:  ['judos', 'kudos', 'kudzu', 'oddly', 'pudgy', 'ruddy', 'sadly', 'scrod', 'sedge', 'sedum', 'sided', 'sprog', 'sudsy', 'suers', 'suets', 'sugar', 'suing', 'suite', 'supra', 'suras', 'surds']\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Choose Nclust cluster centers with uniform random probability\n",
    "import random\n",
    "import string\n",
    "\n",
    "cluster_centers = [random.choice(five_letter_words) for _ in range(Nclust)]\n",
    "print(\"Query word: \", cluster_centers[0])\n",
    "\n",
    "# Step 2. Mask along residue dimension\n",
    "for res in range(5):\n",
    "    if random.randint(1,100) <= 15:  # 15% chance of residue position being included in mask\n",
    "        for pos, word in enumerate(cluster_centers):\n",
    "            if random.randint(1,100) <= 70:  # 70% chance of masking residue in a given word\n",
    "                new_word = word[0:res] + '<masked_msa_token>' + word[res+1:]\n",
    "                cluster_centers[pos] = new_word\n",
    "            elif random.randint(1,100) <= 10:  # 10% chance of mutating residue in a given word\n",
    "                new_word = word[0:res] + random.choice(string.ascii_letters.lower()) + word[res+1:] \n",
    "                cluster_centers[pos] = new_word\n",
    "            # 20% chance of doing nothing\n",
    "            \n",
    "# Step 3. Assign remaining sequences to nearest cluster center\n",
    "def Hamming(query, center):\n",
    "    \"\"\" Computes the number of unmatched residues between query\n",
    "        and center, not including '<masked_msa_token>' \"\"\"\n",
    "    count = 0\n",
    "    for res in range(len(query)):\n",
    "        if query[res] != center[res]:\n",
    "            if center[res] == '<masked_msa_token>':\n",
    "                pass\n",
    "            else:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "center_to_cluster = {key: [] for key in cluster_centers}\n",
    "sequence_to_center = {key: '' for key in five_letter_words if key not in cluster_centers}\n",
    "for sequence in sequence_to_center.keys():\n",
    "    nearest = 6\n",
    "    candidate_centers = []\n",
    "    for center in cluster_centers:\n",
    "        distance = Hamming(sequence, center)\n",
    "        if distance < nearest:\n",
    "            nearest = distance\n",
    "            candidate_centers = [center]\n",
    "        elif distance == nearest:\n",
    "            candidate_centers.append(center)\n",
    "        # otherwise, not a candidate center\n",
    "\n",
    "    assignment = random.choice(candidate_centers)  # choose randomly between equally nearest cluster centers\n",
    "    center_to_cluster[assignment].append(sequence)\n",
    "    sequence_to_center[sequence] = assignment\n",
    "\n",
    "print(\"Query word's cluster: \", center_to_cluster[cluster_centers[0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Compute per-residue statistical features for each cluster\n",
    "amino_acids = [el for el in string.ascii_letters[0:26]] + ['<masked_msa_token>']\n",
    "\n",
    "features_per_cluster = {center: {} for center in cluster_centers}\n",
    "for center in cluster_centers:\n",
    "    # dict of statistical features indexed by residue position\n",
    "    statistical_features = {key: {aa: 0 for aa in amino_acids} for key in range(5)}\n",
    "\n",
    "    for res in range(5):  # iterate through residue positions\n",
    "        for sequence in center_to_cluster[center]:  # iterate through sequences in a cluster\n",
    "            aa = sequence[res]\n",
    "            statistical_features[res][aa] += 1\n",
    "\n",
    "    features_per_cluster[center] = statistical_features\n",
    "\n",
    "# Step 5. An additional Nextra_seq are selected for input to the model, in addition to the cluster centers\n",
    "MSA_input = cluster_centers\n",
    "target_length = min(Nextra_seq + len(cluster_centers), len(five_letter_words) - len(cluster_centers))\n",
    "while(len(MSA_input) < target_length):\n",
    "    candidate_sequence = random.choice(list(sequence_to_center.keys()))\n",
    "    if candidate_sequence not in MSA_input:\n",
    "        MSA_input.append(candidate_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Evoformer Block\n",
    "\n",
    "![Evoformer](pics/AlphaFold_5.png)\n",
    "\n",
    "The input features for the model will be the sequences listed in MSA_input, plus the list of vectors contained in features_per_cluster. The pair representation, which we have not discussed in too much detail, is also used as input. The MSA representation and extra MSA sequence features are passed through a series of simple transformations (a shallow neural network) to 'embed' them. Included in this embedding is a 'positional' encoding of the residues in the protein sequence.\n",
    "\n",
    "The Evoformer block takes in the embedded MSA representation $\\{\\bf{m}_{si}\\}$ and Pair representation $\\{\\bf{z}_{ij}\\}$, and outputs objects with the same dimensions but which have been highly processed. The algorithm for this processing is as follows:\n",
    "\n",
    "![Algorithm6](pics/AlphaFold_3.png)\n",
    "\n",
    "Below is an example implementation of one of the functions involved in the Evoformer block, the MSA Row-Attention With Pair Bias. Here are some notes on the other functions involved:\n",
    "- MSAColumnAttention() is similar to MSARowAttentionWithPairBias\n",
    "- MSATransition() is a 2-layer neural net which expands the number of channels of the MSA representation\n",
    "- OuterProductMean() takes columns $c_i$ and $c_j$ from the current form of the MSA representation, constructs an outer product, passes this tensor through a linear transformation, and updates the Pair representation at position $i$, $j$ with this object\n",
    "- Triangular multiplicative update is used to update the Pair representation using the triplet of elements from the matrix at coordinates at $ij$, $ik$, and $jk$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of q:  (8, 32, 15, 5)\n",
      "Shape of bias:  (8, 5, 5)\n",
      "Shape of gate:  (8, 32, 15, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[2.40620717e-01, 2.58168239e-01, 2.88862477e+00, ...,\n",
       "         5.86423847e+00, 2.44761383e+00, 3.30228062e+00],\n",
       "        [2.87441219e+00, 1.50218657e+00, 1.80428852e+01, ...,\n",
       "         1.89387488e+00, 1.25631746e+01, 1.79627132e+01],\n",
       "        [2.53785355e+00, 1.09704935e+00, 2.26570838e+00, ...,\n",
       "         5.38563470e+00, 4.07125820e+00, 6.43282813e+00],\n",
       "        ...,\n",
       "        [4.29298243e+00, 2.58256149e+00, 3.19258138e+00, ...,\n",
       "         6.99912087e+00, 3.18050866e+00, 5.94646685e+00],\n",
       "        [1.43163335e+00, 1.24332109e+00, 1.65897406e+00, ...,\n",
       "         4.89691864e+00, 2.10718864e-01, 1.93614068e+00],\n",
       "        [4.46290324e+00, 1.71063912e+01, 1.77111791e+01, ...,\n",
       "         2.49652131e+00, 1.78440157e+01, 4.80581656e+01]],\n",
       "\n",
       "       [[1.48688113e+00, 6.48556569e-01, 2.50120552e+00, ...,\n",
       "         3.31244045e+00, 1.67528203e+00, 2.19096645e+00],\n",
       "        [1.95299614e+01, 1.91332993e+01, 5.84188259e+00, ...,\n",
       "         1.51593494e+01, 1.93496591e+01, 4.25268619e+01],\n",
       "        [3.39384357e+01, 1.10326715e+01, 8.83397490e+01, ...,\n",
       "         1.81686004e+02, 9.13039780e+01, 1.70624127e+02],\n",
       "        ...,\n",
       "        [1.68270669e+00, 1.78198512e+00, 5.61093227e-01, ...,\n",
       "         2.04086122e+00, 1.51378491e+00, 4.53910970e-01],\n",
       "        [7.41226151e+00, 2.60646567e-01, 6.57150843e+00, ...,\n",
       "         3.17412152e+00, 1.71618185e+00, 8.93556163e+00],\n",
       "        [1.50318158e+01, 3.30592966e+01, 5.15512571e+01, ...,\n",
       "         4.12146825e+01, 3.69016428e+01, 4.90555793e+01]],\n",
       "\n",
       "       [[5.25455034e+00, 2.05984337e+00, 6.08690056e+00, ...,\n",
       "         8.15392200e-01, 1.98605590e+00, 2.12005683e+00],\n",
       "        [9.22065765e+00, 5.80218510e+00, 2.60033783e+00, ...,\n",
       "         1.60079046e+01, 3.32526399e+00, 1.90981682e+00],\n",
       "        [2.14906837e+01, 9.69243259e+00, 4.86232282e+01, ...,\n",
       "         5.68413559e+01, 5.08767430e+01, 9.09529532e+01],\n",
       "        ...,\n",
       "        [1.05412267e+00, 2.52479145e-01, 6.98018776e-01, ...,\n",
       "         5.12717515e+00, 2.71448266e+00, 4.12382644e+00],\n",
       "        [7.68318699e+01, 2.52960769e+01, 1.17994231e+02, ...,\n",
       "         1.61817275e+02, 9.30361494e+01, 1.67826503e+02],\n",
       "        [1.73794950e+00, 1.02156134e+00, 1.08580134e+00, ...,\n",
       "         2.42388245e+00, 1.11471086e+00, 2.07798152e+00]],\n",
       "\n",
       "       [[5.15965238e-01, 1.91529085e-01, 1.16552410e+00, ...,\n",
       "         4.59393195e-01, 5.65164457e-01, 8.75236519e-01],\n",
       "        [8.91577500e+00, 6.58668841e+00, 3.72202658e+01, ...,\n",
       "         9.59699728e+00, 1.15838965e+01, 1.47830998e-01],\n",
       "        [6.47882801e+00, 2.70624011e+00, 6.77567507e-01, ...,\n",
       "         1.58207967e+01, 7.28186064e+00, 5.96386600e+00],\n",
       "        ...,\n",
       "        [7.38922236e+00, 7.64499119e+00, 1.18840187e+01, ...,\n",
       "         1.43956173e+01, 4.78625929e+00, 1.16494334e+01],\n",
       "        [1.50741644e+00, 1.89244393e+00, 2.15225711e+00, ...,\n",
       "         6.74734089e+00, 1.18905507e+00, 9.87542308e+00],\n",
       "        [3.18769006e+01, 6.85363893e+01, 7.60786626e+01, ...,\n",
       "         6.20051336e+01, 2.30856902e+00, 1.55415844e+01]],\n",
       "\n",
       "       [[9.85969785e-01, 2.09844410e+00, 1.61204899e+00, ...,\n",
       "         2.08020432e+00, 2.51305000e+00, 3.90157530e+00],\n",
       "        [1.95036633e+00, 9.98071098e+00, 4.35292318e+00, ...,\n",
       "         1.65188997e+01, 6.00993339e+00, 3.41805817e+00],\n",
       "        [4.96123625e+00, 1.61060369e+01, 1.34705566e+00, ...,\n",
       "         3.07384497e+01, 2.00874670e+01, 2.06947146e+00],\n",
       "        ...,\n",
       "        [1.38107290e+00, 3.30490973e+00, 6.65365805e-01, ...,\n",
       "         5.79497756e+00, 3.02882428e+00, 6.57279346e+00],\n",
       "        [5.97044694e+00, 1.25328051e+01, 1.18599431e+01, ...,\n",
       "         2.41569410e+01, 3.13274997e+01, 3.99173379e+01],\n",
       "        [7.95880267e+00, 8.33210019e+00, 8.13085124e+00, ...,\n",
       "         1.76956218e+01, 9.31824917e+00, 6.18234502e+00]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "m_test = np.random.random((3, 15, 5)) # (num channels, num species, sequence length)\n",
    "z_test = np.random.random((4, 5, 5)) # (num channels, sequence length, sequence length)\n",
    "\n",
    "\n",
    "# Some auxillary functions which will be useful\n",
    "\n",
    "def LayerNorm(x):\n",
    "    \"\"\" Assumes x is channels first, normalizes within each channel and return normed x \"\"\"\n",
    "    for channel in range(x.shape[0]):\n",
    "        top = np.amax(x[channel])\n",
    "        x[channel] = np.divide(x[channel], top)\n",
    "    return x\n",
    "\n",
    "def Linear(x, A, b=None):\n",
    "    \"\"\" Performs linear transformation of x according to transformation z=dot(x.T, A), possibly with bias b\"\"\"\n",
    "    z = np.dot(x.T, A)\n",
    "    if b != None:\n",
    "        z += b\n",
    "    return z\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(z, idx):\n",
    "    numerator = np.exp(z[idx])\n",
    "    denominator = np.sum(z)\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "# The main function of interest\n",
    "\n",
    "def MSARowAttentionWithPairBias(m, z, c=32, Nhead=8):\n",
    "    # Perform some checks and interpret input dimensions\n",
    "    assert len(m.shape) == 3\n",
    "    assert len(z.shape) == 3\n",
    "    assert (z.shape[1] == z.shape[2]) and (z.shape[1] == m.shape[2])\n",
    "    nMSAfeat = m.shape[0]  # number of MSA features\n",
    "    nMSAseq = m.shape[1]  # number of sequences in the MSA representation\n",
    "    MSAseqlen = m.shape[2]  # the MSA sequence length (may be truncated compared to original query sequence)\n",
    "    nPairFeat = z.shape[0]  # number of pair representation features\n",
    "\n",
    "    # -----------------\n",
    "    # Input projections\n",
    "    # -----------------\n",
    "    m = LayerNorm(m)\n",
    "    #   Query, key, value:\n",
    "    query_kernel = np.random.random((Nhead, nMSAfeat, c))\n",
    "    key_kernel = np.random.random((Nhead, nMSAfeat, c))\n",
    "    value_kernel = np.random.random((Nhead, nMSAfeat, c))\n",
    "    q = np.zeros((Nhead, c, nMSAseq, MSAseqlen))\n",
    "    k = np.zeros((Nhead, c, nMSAseq, MSAseqlen))\n",
    "    v = np.zeros((Nhead, c, nMSAseq, MSAseqlen))\n",
    "    for head in range(Nhead):\n",
    "        qh = Linear(m, query_kernel[head])\n",
    "        kh = Linear(m, key_kernel[head])\n",
    "        vh = Linear(m, value_kernel[head])\n",
    "\n",
    "        q[head] = qh.T\n",
    "        k[head] = kh.T\n",
    "        v[head] = vh.T\n",
    "        \n",
    "    print(\"Shape of q: \", q.shape)  # (Nhead, c, nMSAseq, MSAseqlen)\n",
    "    \n",
    "    #   Bias:\n",
    "    bias_kernel = np.random.random((Nhead, nPairFeat, 1))\n",
    "    b = np.zeros((Nhead, MSAseqlen, MSAseqlen))\n",
    "    for head in range(Nhead):\n",
    "        bhij = Linear(LayerNorm(z), bias_kernel[head]).T\n",
    "        b[head] = bhij\n",
    "    \n",
    "    print(\"Shape of bias: \", b.shape)\n",
    "    \n",
    "    #   Gates:\n",
    "    gate_kernel = np.random.random((Nhead, nMSAfeat, c))\n",
    "    g = np.zeros((Nhead, c, nMSAseq, MSAseqlen))\n",
    "    for head in range(Nhead):\n",
    "        gh = sigmoid(Linear(m, gate_kernel[head]))\n",
    "        g[head] = gh.T\n",
    "    print(\"Shape of gate: \", g.shape)\n",
    "\n",
    "    # -----------------\n",
    "    # Attention\n",
    "    # -----------------\n",
    "    a = np.zeros((Nhead, nMSAseq, MSAseqlen, MSAseqlen))\n",
    "    for head in range(b.shape[0]):\n",
    "        for s in range(nMSAseq):\n",
    "            for i in range(MSAseqlen):\n",
    "                for j in range(MSAseqlen): \n",
    "                    # contract along channel dimension\n",
    "                    for c_idx in range(c):\n",
    "                        a[head][s][i][j] += q[head][c_idx][s][i] * k[head][c_idx][s][j] / np.sqrt(c)\n",
    "                    a[head][s][i][j] += b[head][i][j]\n",
    "    # Apply softmax\n",
    "    for head in range(b.shape[0]):\n",
    "        for s in range(nMSAseq):\n",
    "            for i in range(MSAseqlen):\n",
    "                for j in range(MSAseqlen):\n",
    "                    a[head][s][i][j] = softmax(a[head][s][i][:], j)\n",
    "    # Compute output\n",
    "    o = np.zeros(q.shape)\n",
    "    for head in range(b.shape[0]):\n",
    "        for c_idx in range(c):\n",
    "            for s in range(nMSAseq):\n",
    "                for i in range(MSAseqlen): \n",
    "                    o[head][c_idx][s][i] = g[head][c_idx][s][i] * np.dot(a[head][s][i][:], v[head][c_idx][s][:])\n",
    "    \n",
    "    # Output projection\n",
    "    out = np.concatenate(list([o[h_idx] for h_idx in range(Nhead)]), axis=0)\n",
    "    output_kernel = np.random.random(out.T.shape)\n",
    "    m_tilde = np.multiply(out.T, output_kernel)\n",
    "    return m_tilde\n",
    "    \n",
    "                    \n",
    "MSARowAttentionWithPairBias(m_test, z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triangle updates are emphasized by the authors as particularly important for the success of AlphaFold2.\n",
    "\n",
    "Multiplicative updates using outgoing or incoming edges are fairly straightforward. The difference is just whether the update is done row-wise (outgoing) or column-wise (incoming) from the Pair representation.\n",
    "- element $z_{ij}$ from the Pair representation is updated by taking rows (columns) $i$ and $j$, performing linear projections along the channel dimension, multiplying and rescaling, and adding the resulting vector to element $z_{ij}$\n",
    "\n",
    "Triangular self-attention is very much like the MSA Row Attention but combines the query vector $q_{ij}^h$ with key vector $k_{kj}^h$ to form the attention value for value vector $v_{kj}^h$, and the output is summed over index $k$\n",
    "\n",
    "The extra MSA sequences are treated somewhat differently to the MSA representation, using global attention (i.e., averaged over species) rather than local attention.\n",
    "\n",
    "![TriangleUpdates](pics/AlphaFold_4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
